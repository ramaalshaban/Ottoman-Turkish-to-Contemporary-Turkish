{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('/home/furkan/Desktop/Harfler/ayn_and_be_small_1.h5')\n",
    "model.summary()  # As a reminder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "img_path = '/home/furkan/Desktop/pngs/112.jpg'\n",
    "\n",
    "# We preprocess the image into a 4D tensor\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "# Remember that the model was trained on inputs\n",
    "# that were preprocessed in the following way:\n",
    "#img_tensor /= 255.\n",
    "\n",
    "# Its shape is (1, 150, 150, 3)\n",
    "print(img_tensor.shape)\n",
    "\n",
    "img_tensor =  preprocess_input(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0138a9e2a842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "preds = model.predict_classes(img_tensor)\n",
    "print('Predicted:',preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
